{% extends 'generation/base.html' %}
{% load staticfiles %}

{% block nav-about %} active {% endblock %}

{% block main %}
<main role="main" class="col-md-9 ml-sm-auto col-lg-10">
  <div class="card my-3 mx-5">
    <div class='card-body'>
      <div>
        <h5 class='text-center mb-3'>
          <dt>소설 데이터 기반 문장 생성</dt>
        </h5>
        <p>Input으로 시작단어를 주면 소설데이터를 기반으로 해당 단어로 시작하는 완전한 문장을 생성한다.</p>
        <div class="text-center mb-3">
          <img src="{% static 'images/des_example.png' %}" class="img-thumbnail w-50">
        </div>
        <p>Train 데이터의 문장을 그대로 추출하는 것이 아니라 학습한 데이터를 기반으로 생성하는 모델을 구축하였다. 또한 기존의 연구는 학습이 용이한 영어 데이터를 주로 이용하였으나 본 연구에서는 한국어 소설
          데이터를 사용하였다. 음절 / 형태소 / 어절 세 단위로 나누어 실험을 해 보았으며, 제공하는 데모는 형태소 단위 모델이다.</p>
      </div>

      <div class="mt-5">
        <h5 class='text-center mb-3'>
          <dt>Background</dt>
        </h5>
        <p>한국어는 가장 작게는 음소, 즉 자음과 모음 단위로 이루어져 있다. 음소가 모여서 음절이 되고 음절이 모여 형태소,
          그리고 단어가 된다. 한국어 텍스트를 단어 단위로 생성할 때, 각 단어를 독립적으로 생성하지 말고 선행하는 단어
          가 무엇이냐에 따라 현재의 단어를 생성할 수 있다. 예를 들어 '나는 밥을' 다음에는 '먹었다'가 자주 등장할 것이고,
          ‘나는 물을’ 다음에는 '마셨다'가 자주 등장할 가능성이 높다. 이러한 과정을 수학적으로 모델링한 것이 마르코프 체
          인이다. 직전의 단어 몇 개를 보느냐에 따라서 n-gram 모델을 만들 수 있다. bigram은 직전의 단어 하나가 무엇이
          냐에 따라 현재의 단어를 생성한 것이고 더 말이 되는 문장을 만들고 싶다면 직전 음절 여러개를 보는 '3-gram' 또는
          '4-gram'을 시도해볼 수 있다. 앞서 말한 대로 기존에는 확률 기반의 마르코프 체인 모델 등을 이용한 자연어 생성
          시도가 있었으며 최근에는 딥러닝을 이용한 방향이 활발히 연구중이다. 텍스트 데이터는 순서가 있는 시퀀스 데이
          터이기 때문에 RNN(Recurrent Neural Net) 기반의 모델을 사용하는 경우가 많다.</p>
      </div>

      <div class="mt-5">
        <h5 class='text-center mb-3'>
          <dt>프로젝트 개요 (Project Overveiw)</dt>
        </h5>
        <div class="mb-3">
          <h5>
            <dt>Workflow</dt>
          </h5>
          <p>1. 데이터 수집 및 전처리</p>
          <ul>
            <li>크롤링을 통해 데이터를 수집 (데이터 출처 : 네이버 웹 소설)</li>
            <li>정규식을 이용하여 데이터를 한 문장씩 분리</li>
            <li>학습이 이루어질 수 있는 적절한 길이의 문장을 추출(약 34만 문장)</li>
            <li>konlpy의 Twitter 모듈을 이용하여 형태소 품사 태깅</li>
          </ul>

          <p>2. 단어 벡터 임베딩 및 시각화</p>
          <ul>
            <li>형태소 단위의 모델에 임베딩 벡터 사용</li>
            <li>gensim의 word2vec 모듈 이용</li>
            <li>임베딩 결과 시각화</li>
            <div class="text-center mb-3">
              <img src="{% static 'images/des_tsne.jpg' %}" class="img-thumbnail w-50">
            </div>
          </ul>

          <p>3. LSTM Language Model 구축</p>
          <ul>
            <li>Framework - tensorflow</li>
            <div class="text-center mb-3">
              <img src="{% static 'images/des_model_structure.png' %}" class="img-thumbnail w-75">
            </div>
            <li>Language Model
              <ul>
                <li>문장의 각 시퀀스에 확률을 할당하고, 주어진 문맥상 다음에 나올 적절한 글자, 단어, 또는 문장을
                  예측하는 확률 모델을 의미한다.</li>
                  <li>음성 인식(speech recognition), 기계번역(machine translation), 이미지에 주석달기(image
                    captioning)과 같은 태스크에 널리 이용된다.</li>
              </ul>
            </li>
            <li>모델의 인풋과 아웃풋
              <ul>
                <li><b>Input</b> : 주어진 배치 사이즈의 시퀀스(여기서는 한 문장으로 배치를 할당)</li>
                <li><b>Output</b> : 주어진 시퀀스 이후에 올 수 있는 단어들의 확률</li>                
                <li>아웃풋으로 주어진 단어들의 확률로 분포를 만들어 다음에 올 단어를 선택한다.</li>
              </ul>
            </li>
          </ul>

          <p>4. 평가 척도</p>
          <ul>
            <li>BLEU score
              <ul>
                <li>주로 기계 번역 문에에 사용되는 지표로 [0,1]의 범위 안에 있다, 이 때주어진 정답 문장과 비슷할 수
                    록 높은 값을 가진다.</li>
                <li>generation task에서는 정답 문장이 없으므로, input data와 비슷할수록 높은 값을 가진다.</li>
              </ul>
            </li>
            <li>gensim의 word2vec 모듈 이용</li>
            <li>임베딩 결과 시각화</li>
          </ul>

          <h5>
            <dt>Results</dt>
          </h5>
          <ol>
            <li>음절/형태소/어절 세 모델 중 형태소 단위 모델이 가장 나은 성능을 보였다.</li>
            <li>형태소 단위 모델의 경우 단어안의 형태소 구조를 잘 포착하였으며(예시 : 들어갔다 올게, 전달했냐, 결혼할 수 있어?) 단어간의 연관성도 어느 정도 잘 연결되는 것을 확인할 수 있었다.</li>
            <li>다만, 문장 중간에 콤마가 들어갈 경우 앞뒤로 연결이 어색해지는 문제가 있었다.</li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</main>
{% endblock %}